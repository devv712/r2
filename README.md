Federated Learning: Privacy-Preserving Machine Learning Across Decentralized Devices With Insights From The GDPR Perspective

Abstract: Federated Learning (FL) is a novel machine learning approach enabling decentralized model training while safeguarding data privacy. Unlike traditional centralized methods, FL mitigates privacy risks by retaining data on local devices, addressing challenges such as data security, regulatory compliance, and data heterogeneity. This study highlights FL’s principles, privacy-enhancing techniques, and applications across sectors, emphasizing its alignment with the General Data Protection Regulation (GDPR). Case studies and data visualization tools like Orange illustrate the opportunities and challenges in developing privacy-preserving FL systems.

I. INTRODUCTION
In today’s data-driven world, applications powered by artificial intelligence (AI) and machine learning (ML) play a critical role in industries such as healthcare, finance, and autonomous systems. These applications rely on large-scale data collection and processing, often conducted through centralized ML models that aggregate raw data into centralized servers for training. However, this approach poses significant challenges, including risks of data breaches, compliance issues with stringent regulations such as the EU General Data Protection Regulation (GDPR), and the erosion of user trust due to limited transparency in data handling.
Federated Learning (FL), introduced by Google researchers in 2016, presents a transformative paradigm for addressing these challenges. By enabling collaborative training across decentralized devices, FL eliminates the need to transfer raw data to central servers. Instead, model training occurs locally on user devices, and only model updates are shared with a central server or a peer-to-peer network for aggregation. This decentralized approach inherently enhances data privacy, reduces security risks, and offers a path toward GDPR compliance by keeping personal data on local devices. FL has opened new opportunities for service providers to deploy privacy-preserving ML algorithms without directly accessing users’ sensitive data.
Despite its potential, FL is not immune to privacy risks. Research has shown that adversaries can exploit exchanged model parameters to infer sensitive information, posing challenges for strict compliance with data protection regulations. Privacy-preserving techniques, including cryptographic methods and differential privacy, have been developed to mitigate such risks and strengthen the security of FL systems. However, implementing fully GDPR-compliant FL systems remains a complex and evolving task.
This paper aims to provide a comprehensive exploration of FL from a privacy-preservation perspective, particularly concerning GDPR compliance. It discusses the limitations of traditional centralized ML approaches, evaluates state-of-the-art privacy-preserving techniques in FL, and examines how these
www.ijcrt.org © 2024 IJCRT | Volume 12, Issue 12 December 2024 | ISSN: 2320-2882
IJCRT2412872 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org h867
techniques can address data security challenges. Additionally, the study highlights unresolved issues and potential future directions to align FL systems with regulatory frameworks, ultimately contributing to the development of secure and trustworthy ML applications.
